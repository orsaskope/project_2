# Πειραματική Ανάλυση — Build and Search Phase (Neural LSH)

Πραγματοποιούμε μια συστηματική πειραματική αξιολόγηση του Neural LSH, εξετάζοντας πώς οι βασικές παράμετροι επηρεάζουν:

1.την ποιότητα της διαμέρισης του γράφου μέσω KaHIP (Build Phase),
την ικανότητα του MLP να μάθει σωστά την αντιστοίχιση των vectors στα blocks,

2.την ακρίβεια εύρεσης κοντινότερων γειτόνων (Approximate Nearest Neighbor Search),

3.την ταχύτητα και ακρίβεις της διαδικασίας αναζήτησης (Search Phase).

Για κάθε run συλλέγουμε συγκεκριμένα metrics που αποτυπώνουν την ποιότητα τόσο του partitioning όσο και της αναζήτησης.

Καταγράφουμε τα εξής metrics για κάθε run:

## Metrics Build Phase
- **Edgecut**: πλήθος ακμών που κόπηκαν από το KaHIP  
- **Min Block Size**: μέγεθος μικρότερου block  
- **Max Block Size**: μέγεθος μεγαλύτερου block  
- **Std Deviation**: η τυπική απόκλιση του μεγέθους των blocks. 
- **Final MLP Loss**: loss της τελευταίας εποχής

## Metrics Search Phase

Για τη φάση αναζήτησης καταγράφονται:

- **AF (Approximation Factor)** – Ιδανικά κοντά στο 1.
- **Recall@N** – Ποσοστό queries όπου ο πραγματικός NN βρίσκεται στους top-N.
- **QPS (Queries Per Second)** – Πόσα queries επεξεργάζεται το σύστημα ανά δευτερόλεπτο.
- **tApproxAvg / tTrueAvg** – Μέσος χρόνος approximate και brute-force αναζήτησης.
- Το πλήθος εξερευνούμενων bins T και το πλήθος επιστρεφόμενων neighbors N.


---


## Πίνακας A — Effect of m (αριθμός partitions)
Σταθερές παράμετροι: kNN = 15, layers = 3, nodes = 256

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 15 | 3 | 256 | 160834 | 752 | 1235 | 90.50657 | 0.2258 |
| 2 | 100 | 15 | 3 | 256 | 229049 | 109 | 617 | 65.35870 | 0.3774 |
| 3 | 150 | 15 | 3 | 256 | 268655 | 6 | 411 | 55.742024 | 0.4615 |


## Πίνακας B — Effect of kNN (μέγεθος k-NN graph)
Σταθερές παράμετροι: m = 50, layers = 3, nodes = 128

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 5 | 3 | 128 | 9486 | 167 | 205 | 8.15107 | 0.9123 |
| 2 | 50 | 10 | 3 | 128 | 25712 | 130 | 205 | 11.40350 | 0.862 |
| 3 | 50 | 15 | 3 | 128 | 45436 | 139 | 205 | 10.96722 | 0.8853 |

## Πίνακας C — Effect of Layers (MLP architecture)
Σταθερές παράμετροι: m = 100, kNN = 15

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 100 | 15 | 2 | 128 | 57519 | 81 | 102 | 3.865 | 0.9940 |
| 2 | 100 | 15 | 3 | 128 | 57519 | 81 | 102 | 3.86522 | 1.2092 |
| 3 | 100 | 15 | 4 | 128 | 57519 | 81 | 102 | 3.86522 | 1.4139 |

## 📊 Πίνακας D — Effect of Hidden Layer Size (Nodes)

Σταθερές παράμετροι: m = 50, kNN = 15, layers = 3
| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 15 | 3 | 64 | 57519 | 81 | 102 | 3.86522 | 1.6357 |
| 2 | 50 | 15 | 3 | 128 | 57519 | 81 | 102 | 3.86522 | 1.2241 |
| 3 | 50 | 15 | 3 | 256 | 57519 | 81 | 102 | 3.86522 | 0.9343 |

---
## SEARCH PHASE
## Πίνακας A — Effect of N (requested neighbors)

Σταθερές παράμετροι: T = 5

| Run | N | AF | Recall@N | QPS | tApproxAvg | tTrueAvg |
|-----|---|------|------------|--------|-------------|-------------|
| 1 | 1  | 1.001247640490532 | 0.982 | 86.88012550927556 | 0.011510112285614015 | 0.0816549460887909 |
| 2 | 5  | 1.001247640490532 | 0.982 | 87.1172953307351 | 0.011478776931762696 | 0.08060728549957276 |
| 3 | 10 | 1.001247640490532 | 0.982 | 80.55264249308684 | 0.012414242029190064 | 0.08017755174636841 |

## Πίνακας B — Effect of T (bins to probe)

Σταθερές παράμετροι: N = 5

| Run | T | AF | Recall@10 | QPS | tApproxAvg | tTrueAvg |
|-----|----|--------|--------------|--------|----------------|----------------|
| 1 | 1  | 1.0309314022660256 | 0.706 | 464.2940679053365 | 0.0021538074016571045 | 0.08074579167366028 |
| 2 | 3  | 1.0046305782198905 | 0.944 | 145.82077325369423 | 0.006857733488082886 | 0.08126531863212585 |
| 3 | 5  | 1.001247640490532 | 0.982 | 65.42223176118966 | 0.015285323858261108 | 0.09682433867454529 |
| 4 | 10 |  1.0002791639566422 | 0.995 | 38.81462112799699 |  0.025763487339019776 | 0.10823445940017701 |

---

## Παρατηρήσεις – Συμπεράσματα
Για το build phase. Τα πρώτα 3 runs (Πίνακας Α) έγιναν με dataset mnist 60.000, ενώ τα runs για τους πίνακες B,C,D έγιναν με dataset mnist 10.000 vectors για λόγους ταχύτητας. Στο search phase όλες οι εκτελέσεις έγιναν με dataset 60.000 vectors

## Συμπεράσματα από τον Πίνακα A — Effect of m
Η αύξηση του αριθμού των partitions **m** έχει ξεκάθαρη και προβλέψιμη επίδραση:

- **Το edgecut αυξάνεται** σημαντικά όσο μεγαλώνει το m, επειδή ο KaHIP αναγκάζεται να σπάσει περισσότερες ακμές για να δημιουργήσει λεπτότερη διαμέριση.
- **Τα block sizes γίνονται πιο ισορροπημένα** όταν αυξάνεται το m.  
  - Για m=50 έχουμε block sizes από 752 έως 1235, δηλαδή μεγάλη διακύμανση.  
  - Για m=150 η διακύμανση πέφτει θεαματικά (από 6 έως 411), άρα το partitioning είναι πιο ομοιόμορφο.
- Η **τυπική απόκλιση** μειώνεται όσο αυξάνεται το m → η διαμέριση είναι πιο balanced.
- Το **final loss του MLP αυξάνεται** όταν αυξάνεται το m.  
  Αυτό είναι αναμενόμενο: περισσότερα blocks άρα δυσκολότερο classification.

**Συμπέρασμα:**  
Υπάρχει trade-off μεταξύ καλής διαμέρισης (ισορροπίας) και ακρίβειας του MLP.  
Το m=100 φαίνεται μια καλή ισορροπία: αρκετά balanced blocks χωρίς υπερβολική αύξηση του loss.

---

## Συμπεράσματα από τον Πίνακα B — Effect of kNN

Η αύξηση του k στο k-NN graph:

- Αυξάνει το **edgecut**, επειδή ο γράφος έχει περισσότερες ακμές, άρα περισσότερες που μπορούν να «κοπούν».
- Δεν επηρεάζει τόσο τα block sizes, διότι ο αριθμός partitions μένει ίδιος (m=50).
- Το final loss **δεν βελτιώνεται μονοτονικά**:
  - k=5 → loss πολύ υψηλότερο
  - k=10 → χαμηλότερο loss
  - k=15 → ελαφρώς υψηλότερο από k=10

Αυτό δείχνει ότι το MLP επωφελείται από έναν πιο πλούσιο γράφο, αλλά η υπερβολική πυκνότητα δεν βελτιώνει αναγκαστικά την πρόβλεψη των blocks.

**Συμπέρασμα:**  
Το k=10 αποτελεί τη βέλτιστη επιλογή εδώ, προσφέροντας χαμηλό loss και αποδεκτό edgecut/cost.

---
## Συμπεράσματα Πίνακα C — Effect of Layers

Με σταθερά m=100 και k=15 εξετάζουμε τον αριθμό των layers:

- Το edgecut και τα block sizes **μένουν ίδια**, όπως αναμένεται (μόνο το MLP αλλάζει).
- Το MLP loss **αυξάνεται όσο προστίθενται περισσότερα layers**:
  - 2 layers → loss 0.994
  - 3 layers → loss 1.2092
  - 4 layers → loss 1.4139

Αυτό είναι αντιστρατευόμενο με το τυπικό intuition (ότι περισσότερα layers μοντελοποιούν καλύτερα τα δεδομένα), αλλά εδώ:

- το task είναι **πολυταξινόμηση σε 100 blocks**,  
- ένα deeper δίκτυο **δυσκολεύεται να γενικεύσει στις KaHIP labels**, που δεν είναι φυσικές κατηγορίες.

**Συμπέρασμα:**  
Το MLP με 2 layers είναι καλύτερο για m=100, δίνοντας χαμηλότερο loss και άρα καλύτερη μάθηση της διαμέρισης.

---
## Συμπεράσματα Πίνακας D — Effect of Nodes

Με σταθερά m=50, k=15, layers=3:

- Αυξάνοντας το hidden size:
  - Nodes 64 → loss 1.6357
  - Nodes 128 → loss 1.2241
  - Nodes 256 → loss 0.9343

Άρα το μοντέλο πράγματι ωφελείται από μεγαλύτερη χωρητικότητα.  
Το edgecut είναι ίδιο, άρα τα αποτελέσματα αντικατοπτρίζουν **μόνο τη μάθηση του MLP**.

**Συμπέρασμα:**  
Μεγαλύτερος αριθμός nodes βελτιώνει σημαντικά την ποιότητα ταξινόμησης των blocks.


---
#  SEARCH PHASE — Συμπεράσματα

## Πίνακας Α — Effect of N (requested neighbors)

Παρατηρείται ότι:

- Το AF παραμένει σταθερό (1.001), ανεξάρτητο του N.
- Το Recall@N είναι 0.982 και για N=1 και για N=10 → το μοντέλο βρίσκει τους πραγματικούς NN σχεδόν πάντα.
- Ο QPS μειώνεται ελαφρώς όσο αυξάνεται το N, λογικό, περισσότερα αποτελέσματα προς επιστροφή.

**Συμπέρασμα:**  
Η επιλογή του N **δεν επηρεάζει ουσιαστικά την ακρίβεια**. Το Neural LSH βρίσκει σωστά τον NN σχεδόν σε όλες τις περιπτώσεις.

---

## Πίνακας Β — Effect of T (bins to probe)

Εδώ φαίνεται καθαρά το trade-off:

- **T=1 → Πάρα πολύ γρήγορο** (QPS ~ 464) αλλά κακό Recall (0.706).
- **T=3 → Μεγάλη βελτίωση Recall (0.944), μικρότερη ταχύτητα.**
- **T=5 → Recall 0.982, QPS ~65**.
- **T=10 → Σχεδόν τέλεια ακρίβεια (0.995), αλλά QPS πέφτει στο ~38**.

**Συμπέρασμα:**  
Το T=5 προσφέρει **το καλύτερο overall trade-off**:  
πολύ υψηλή ακρίβεια με αποδεκτή ταχύτητα.
---

# Τελικό Γενικό Συμπέρασμα
Με βάση όλα τα παραπάνω:

- Το Neural LSH επιτυγχάνει **εξαιρετική ακρίβεια (Recall ~0.98–0.995)** με κατάλληλες παραμέτρους.
- Η ποιότητα του partition από το KaHIP παίζει κρίσιμο ρόλο.
- Οι καλύτερες ρυθμίσεις που προκύπτουν από το σύνολο των πειραμάτων είναι:

### Build Phase  
- **m = 100**  
- **kNN = 10 ή 15**  
- **layers = 2 ή 3**  
- **nodes = 256**

### Search Phase  
- **T = 5** (ισορροπία ακρίβειας/ταχύτητας)  
- **N = 5 ή 10** (καμία ουσιαστική επίδραση στην ακρίβεια)

Συνολικά, ο Neural LSH εμφανίζεται **σαφώς ανώτερος** από τους αλγορίθμους της 1ης εργασίας τόσο σε ακρίβεια όσο και σε χρόνο αναζήτησης.
---

Φτάνουμε σε αυτό το συμπέρασμα καθώς αν αναλύσουμε τα παρακάτω αποτελέσματα της 1ης εργασίας, τα οποία ήταν τα βέλτιστα για κάθε αλγόριθμο:

---

## 1. LSH on MNIST
| w | L | k | Average AF | Recall@N | QPS | tApproximate | tTrue |
|------:|--:|-------:|--:|-----------:|---------:|----:|-------------:|
4 | 15 | 2 | 1.006404 | 0.860000 | 11.210820 | 0.089200 | 0.245247

## 2. Hypercube on MNIST
| kproj | M | probes | w | Average AF | Recall@N | QPS | tApproximate | tTrue |
|------:|--:|-------:|--:|-----------:|---------:|----:|-------------:|------:|
18 | 6000 | 300 | 6 | 1.030866 | 0.700000 | 61.852808 | 0.016167 | 0.238871

## 3. IVFFLAT — MNIST
| N | kclusters | nprobe | seed | Average AF | Recall@N | QPS | tApproximateAverage (s) | tTrueAverage (s) |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 5 | 32 | 4 | 9 | 1.000000 | **0.978** | 20.475630 | 0.048839 | 0.364775 |

Παρατηρουμε οτι το βελτιστο του nlsh

## NLSH — MNIST
| Run | T | AF | Recall@10 | QPS | tApproxAvg | tTrueAvg |
|-----|----|--------|--------------|--------|----------------|----------------|
| 3 | 5  | 1.001247640490532 | 0.982 | 65.42223176118966 | 0.015285323858261108 | 0.09682433867454529 |

* Ανώτερο του LSH σε recall και ταχυτητα

* Ανώτερο του hypercube σε recall και ταχυτητα

* Ανώτερο του ivfflat σε recall και ταχυτητα

Επομένως, ο Neural LSH εμφανίζεται **σαφώς ανώτερος** από τους αλγορίθμους της 1ης εργασίας τόσο σε ακρίβεια όσο και σε χρόνο αναζήτησης.
---