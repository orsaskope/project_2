# Πειραματική Ανάλυση — Build and Search Phase (Neural LSH)

Πραγματοποιούμε μια συστηματική πειραματική αξιολόγηση του Neural LSH, εξετάζοντας πώς οι βασικές παράμετροι επηρεάζουν:

1.την ποιότητα της διαμέρισης του γράφου μέσω KaHIP (Build Phase),
την ικανότητα του MLP να μάθει σωστά την αντιστοίχιση των vectors στα blocks,

2.την ακρίβεια εύρεσης κοντινότερων γειτόνων (Approximate Nearest Neighbor Search),

3.την ταχύτητα και ακρίβεις της διαδικασίας αναζήτησης (Search Phase).

Για κάθε run συλλέγουμε συγκεκριμένα metrics που αποτυπώνουν την ποιότητα τόσο του partitioning όσο και της αναζήτησης.

Καταγράφουμε τα εξής metrics για κάθε run:

## Metrics Build Phase
- **Edgecut**: πλήθος ακμών που κόπηκαν από το KaHIP  
- **Min Block Size**: μέγεθος μικρότερου block  
- **Max Block Size**: μέγεθος μεγαλύτερου block  
- **Std Deviation**: η τυπική απόκλιση του μεγέθους των blocks. 
- **Final MLP Loss**: loss της τελευταίας εποχής

## Metrics Search Phase

Για τη φάση αναζήτησης καταγράφονται:

- **AF (Approximation Factor)** – Ιδανικά κοντά στο 1.
- **Recall@N** – Ποσοστό queries όπου ο πραγματικός NN βρίσκεται στους top-N.
- **QPS (Queries Per Second)** – Πόσα queries επεξεργάζεται το σύστημα ανά δευτερόλεπτο.
- **tApproxAvg / tTrueAvg** – Μέσος χρόνος approximate και brute-force αναζήτησης.
- Το πλήθος εξερευνούμενων bins T και το πλήθος επιστρεφόμενων neighbors N.


---

# MNIST

## Πίνακας A — Effect of m (αριθμός partitions)
Σταθερές παράμετροι: kNN = 15, layers = 3, nodes = 256

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 15 | 3 | 256 | 160834 | 752 | 1235 | 90.50657 | 0.2258 |
| 2 | 100 | 15 | 3 | 256 | 229049 | 109 | 617 | 65.35870 | 0.3774 |
| 3 | 150 | 15 | 3 | 256 | 268655 | 6 | 411 | 55.742024 | 0.4615 |


## Πίνακας B — Effect of kNN (μέγεθος k-NN graph)
Σταθερές παράμετροι: m = 50, layers = 3, nodes = 128

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 5 | 3 | 128 | 9486 | 167 | 205 | 8.15107 | 0.9123 |
| 2 | 50 | 10 | 3 | 128 | 25712 | 130 | 205 | 11.40350 | 0.862 |
| 3 | 50 | 15 | 3 | 128 | 45436 | 139 | 205 | 10.96722 | 0.8853 |

## Πίνακας C — Effect of Layers (MLP architecture)
Σταθερές παράμετροι: m = 100, kNN = 15

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 100 | 15 | 2 | 128 | 57519 | 81 | 102 | 3.865 | 0.9940 |
| 2 | 100 | 15 | 3 | 128 | 57519 | 81 | 102 | 3.86522 | 1.2092 |
| 3 | 100 | 15 | 4 | 128 | 57519 | 81 | 102 | 3.86522 | 1.4139 |

## 📊 Πίνακας D — Effect of Hidden Layer Size (Nodes)

Σταθερές παράμετροι: m = 50, kNN = 15, layers = 3
| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 15 | 3 | 64 | 57519 | 81 | 102 | 3.86522 | 1.6357 |
| 2 | 50 | 15 | 3 | 128 | 57519 | 81 | 102 | 3.86522 | 1.2241 |
| 3 | 50 | 15 | 3 | 256 | 57519 | 81 | 102 | 3.86522 | 0.9343 |

---
## SEARCH PHASE
## Πίνακας A — Effect of N (requested neighbors)

Σταθερές παράμετροι: T = 5

| Run | N | AF | Recall@N | QPS | tApproxAvg | tTrueAvg |
|-----|---|------|------------|--------|-------------|-------------|
| 1 | 1  | 1.001247640490532 | 0.982 | 86.88012550927556 | 0.011510112285614015 | 0.0816549460887909 |
| 2 | 5  | 1.001247640490532 | 0.982 | 87.1172953307351 | 0.011478776931762696 | 0.08060728549957276 |
| 3 | 10 | 1.001247640490532 | 0.982 | 80.55264249308684 | 0.012414242029190064 | 0.08017755174636841 |

## Πίνακας B — Effect of T (bins to probe)

Σταθερές παράμετροι: N = 5

| Run | T | AF | Recall@10 | QPS | tApproxAvg | tTrueAvg |
|-----|----|--------|--------------|--------|----------------|----------------|
| 1 | 1  | 1.0309314022660256 | 0.706 | 464.2940679053365 | 0.0021538074016571045 | 0.08074579167366028 |
| 2 | 3  | 1.0046305782198905 | 0.944 | 145.82077325369423 | 0.006857733488082886 | 0.08126531863212585 |
| 3 | 5  | 1.001247640490532 | 0.982 | 65.42223176118966 | 0.015285323858261108 | 0.09682433867454529 |
| 4 | 10 |  1.0002791639566422 | 0.995 | 38.81462112799699 |  0.025763487339019776 | 0.10823445940017701 |

---

## Παρατηρήσεις – Συμπεράσματα
Για το build phase. Τα πρώτα 3 runs (Πίνακας Α) έγιναν με dataset mnist 60.000, ενώ τα runs για τους πίνακες B,C,D έγιναν με dataset mnist 10.000 vectors για λόγους ταχύτητας. Στο search phase όλες οι εκτελέσεις έγιναν με dataset 60.000 vectors

## Συμπεράσματα από τον Πίνακα A — Effect of m
Η αύξηση του αριθμού των partitions **m** έχει ξεκάθαρη και προβλέψιμη επίδραση:

- **Το edgecut αυξάνεται** σημαντικά όσο μεγαλώνει το m, επειδή ο KaHIP αναγκάζεται να σπάσει περισσότερες ακμές για να δημιουργήσει λεπτότερη διαμέριση.
- **Τα block sizes γίνονται πιο ισορροπημένα** όταν αυξάνεται το m.  
  - Για m=50 έχουμε block sizes από 752 έως 1235, δηλαδή μεγάλη διακύμανση.  
  - Για m=150 η διακύμανση πέφτει θεαματικά (από 6 έως 411), άρα το partitioning είναι πιο ομοιόμορφο.
- Η **τυπική απόκλιση** μειώνεται όσο αυξάνεται το m → η διαμέριση είναι πιο balanced.
- Το **final loss του MLP αυξάνεται** όταν αυξάνεται το m.  
  Αυτό είναι αναμενόμενο: περισσότερα blocks άρα δυσκολότερο classification.

**Συμπέρασμα:**  
Υπάρχει trade-off μεταξύ καλής διαμέρισης (ισορροπίας) και ακρίβειας του MLP.  
Το m=100 φαίνεται μια καλή ισορροπία: αρκετά balanced blocks χωρίς υπερβολική αύξηση του loss.

---

## Συμπεράσματα από τον Πίνακα B — Effect of kNN

Η αύξηση του k στο k-NN graph:

- Αυξάνει το **edgecut**, επειδή ο γράφος έχει περισσότερες ακμές, άρα περισσότερες που μπορούν να «κοπούν».
- Δεν επηρεάζει τόσο τα block sizes, διότι ο αριθμός partitions μένει ίδιος (m=50).
- Το final loss **δεν βελτιώνεται μονοτονικά**:
  - k=5 → loss πολύ υψηλότερο
  - k=10 → χαμηλότερο loss
  - k=15 → ελαφρώς υψηλότερο από k=10

Αυτό δείχνει ότι το MLP επωφελείται από έναν πιο πλούσιο γράφο, αλλά η υπερβολική πυκνότητα δεν βελτιώνει αναγκαστικά την πρόβλεψη των blocks.

**Συμπέρασμα:**  
Το k=10 αποτελεί τη βέλτιστη επιλογή εδώ, προσφέροντας χαμηλό loss και αποδεκτό edgecut/cost.

---
## Συμπεράσματα Πίνακα C — Effect of Layers

Με σταθερά m=100 και k=15 εξετάζουμε τον αριθμό των layers:

- Το edgecut και τα block sizes **μένουν ίδια**, όπως αναμένεται (μόνο το MLP αλλάζει).
- Το MLP loss **αυξάνεται όσο προστίθενται περισσότερα layers**:
  - 2 layers → loss 0.994
  - 3 layers → loss 1.2092
  - 4 layers → loss 1.4139

Αυτό είναι αντιστρατευόμενο με το τυπικό intuition (ότι περισσότερα layers μοντελοποιούν καλύτερα τα δεδομένα), αλλά εδώ:

- το task είναι **πολυταξινόμηση σε 100 blocks**,  
- ένα deeper δίκτυο **δυσκολεύεται να γενικεύσει στις KaHIP labels**, που δεν είναι φυσικές κατηγορίες.

**Συμπέρασμα:**  
Το MLP με 2 layers είναι καλύτερο για m=100, δίνοντας χαμηλότερο loss και άρα καλύτερη μάθηση της διαμέρισης.

---
## Συμπεράσματα Πίνακας D — Effect of Nodes

Με σταθερά m=50, k=15, layers=3:

- Αυξάνοντας το hidden size:
  - Nodes 64 → loss 1.6357
  - Nodes 128 → loss 1.2241
  - Nodes 256 → loss 0.9343

Άρα το μοντέλο πράγματι ωφελείται από μεγαλύτερη χωρητικότητα.  
Το edgecut είναι ίδιο, άρα τα αποτελέσματα αντικατοπτρίζουν **μόνο τη μάθηση του MLP**.

**Συμπέρασμα:**  
Μεγαλύτερος αριθμός nodes βελτιώνει σημαντικά την ποιότητα ταξινόμησης των blocks.


---
#  SEARCH PHASE — Συμπεράσματα

## Πίνακας Α — Effect of N (requested neighbors)

Παρατηρείται ότι:

- Το AF παραμένει σταθερό (1.001), ανεξάρτητο του N.
- Το Recall@N είναι 0.982 και για N=1 και για N=10 → το μοντέλο βρίσκει τους πραγματικούς NN σχεδόν πάντα.
- Ο QPS μειώνεται ελαφρώς όσο αυξάνεται το N, λογικό, περισσότερα αποτελέσματα προς επιστροφή.

**Συμπέρασμα:**  
Η επιλογή του N **δεν επηρεάζει ουσιαστικά την ακρίβεια**. Το Neural LSH βρίσκει σωστά τον NN σχεδόν σε όλες τις περιπτώσεις.

---

## Πίνακας Β — Effect of T (bins to probe)

Εδώ φαίνεται καθαρά το trade-off:

- **T=1 → Πάρα πολύ γρήγορο** (QPS ~ 464) αλλά κακό Recall (0.706).
- **T=3 → Μεγάλη βελτίωση Recall (0.944), μικρότερη ταχύτητα.**
- **T=5 → Recall 0.982, QPS ~65**.
- **T=10 → Σχεδόν τέλεια ακρίβεια (0.995), αλλά QPS πέφτει στο ~38**.

**Συμπέρασμα:**  
Το T=5 προσφέρει **το καλύτερο overall trade-off**:  
πολύ υψηλή ακρίβεια με αποδεκτή ταχύτητα.
---

# Τελικό Γενικό Συμπέρασμα
Με βάση όλα τα παραπάνω:

- Το Neural LSH επιτυγχάνει **εξαιρετική ακρίβεια (Recall ~0.98–0.995)** με κατάλληλες παραμέτρους.
- Η ποιότητα του partition από το KaHIP παίζει κρίσιμο ρόλο.
- Οι καλύτερες ρυθμίσεις που προκύπτουν από το σύνολο των πειραμάτων είναι:

### Build Phase  
- **m = 100**  
- **kNN = 10 ή 15**  
- **layers = 2 ή 3**  
- **nodes = 256**

### Search Phase  
- **T = 5** (ισορροπία ακρίβειας/ταχύτητας)  
- **N = 5 ή 10** (καμία ουσιαστική επίδραση στην ακρίβεια)

Συνολικά, ο Neural LSH εμφανίζεται **σαφώς ανώτερος** από τους αλγορίθμους της 1ης εργασίας τόσο σε ακρίβεια όσο και σε χρόνο αναζήτησης σε mnist data.
---

Φτάνουμε σε αυτό το συμπέρασμα καθώς αν αναλύσουμε τα παρακάτω αποτελέσματα της 1ης εργασίας, τα οποία ήταν τα βέλτιστα για κάθε αλγόριθμο:

---

## 1. LSH on MNIST
| w | L | k | Average AF | Recall@N | QPS | tApproximate | tTrue |
|------:|--:|-------:|--:|-----------:|---------:|----:|-------------:|
4 | 15 | 2 | 1.006404 | 0.860000 | 11.210820 | 0.089200 | 0.245247

## 2. Hypercube on MNIST
| kproj | M | probes | w | Average AF | Recall@N | QPS | tApproximate | tTrue |
|------:|--:|-------:|--:|-----------:|---------:|----:|-------------:|------:|
18 | 6000 | 300 | 6 | 1.030866 | 0.700000 | 61.852808 | 0.016167 | 0.238871

## 3. IVFFLAT — MNIST
| N | kclusters | nprobe | seed | Average AF | Recall@N | QPS | tApproximateAverage (s) | tTrueAverage (s) |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 5 | 32 | 4 | 9 | 1.000000 | **0.978** | 20.475630 | 0.048839 | 0.364775 |

## 4. IVFPQ — MNIST
| nbits | kclusters | nprobe | seed | Average AF | Recall@N | QPS | tApproximateAverage (s) | tTrueAverage (s) |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 8 | 16 | 4 | 9 | 1.081521 | **0.608000** | 53.279835 | 0.018769 | 0.447600 |



Παρατηρουμε οτι στο βελτιστο του nlsh:
---

## NLSH — MNIST
| Run | T | AF | Recall@10 | QPS | tApproxAvg | tTrueAvg |
|-----|----|--------|--------------|--------|----------------|----------------|
| 3 | 5  | 1.001247640490532 | 0.982 | 65.42223176118966 | 0.015285323858261108 | 0.09682433867454529 |

* Ανώτερο του LSH σε recall και ταχυτητα

* Ανώτερο του hypercube σε recall και ταχυτητα

* Ανώτερο του ivfflat σε recall και ταχυτητα

* Ανώτερο του ivfpq σε recall και ταχυτητα

Επομένως, ο Neural LSH εμφανίζεται **σαφώς ανώτερος** από τους αλγορίθμους της 1ης εργασίας τόσο σε ακρίβεια όσο και σε χρόνο αναζήτησης στο mnist.

# SIFT

Στο sift εχουμε χρησιμοποποιησει dataset μηκους 10000 vectors για input λογω δυσκολιας του υπολογιστη να ανταπεξελθει στο 1000000.Αυτο αιτιολογει καποιους παραξεννους αριθμους οπως τα ttrue average.

## Πίνακας A — Effect of m (αριθμός partitions)
Σταθερές παράμετροι: kNN = 15, layers = 3, nodes = 256

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 5 | 3 | 128 | 15377 | 144 | 205 | 12.789057822998535 | 1.5290 |
| 2 | 100 | 5 | 3 | 128 | 18470 | 1 | 102 | 10.76475 | 2.1199 |
| 3 | 150 | 5 | 3 | 128 | 20370 | 12 | 68 | 5.512007 | 2.5122 |


## Πίνακας B — Effect of kNN (μέγεθος k-NN graph)
Σταθερές παράμετροι: m = 50, layers = 3, nodes = 128

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 5 | 3 | 128 | 15377 | 144 | 205 | 12.789057822998535 | 1.4921 |
| 2 | 50 | 10 | 3 | 128 | 37305 | 88 | 205 | 17.74823934929885 | 1.3609 |
| 3 | 50 | 15 | 3 | 128 | 61018 | 95 | 205 | 18.716148 | 1.3208 |

## Πίνακας C — Effect of Layers (MLP architecture)
Σταθερές παράμετροι: m = 100, kNN = 15

| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 100 | 15 | 2 | 128 | 74016 | 1 | 102 | 11.480418 | 1.8692 |
| 2 | 100 | 15 | 3 | 128 | 74016 | 1 | 102 | 11.480418 | 1.8692 |
| 3 | 100 | 15 | 4 | 128 | 11.480418 | 1 | 102 | 11.480418 | 2.1411 |

##  Πίνακας D — Effect of Hidden Layer Size (Nodes)

Σταθερές παράμετροι: m = 50, kNN = 15, layers = 3
| Run | m | kNN | Layers | Nodes | Edgecut | Min Block | Max Block | Std Dev | Final Loss |
|-----|----|------|----------|----------|--------------|--------------|--------------|--------------|--------------|
| 1 | 50 | 15 | 3 | 64 | 61018 | 95 | 205 | 18.706148 | 1.7978 |
| 2 | 50 | 15 | 3 | 128 | 61018 | 95 | 205 | 18.706148 | 1.3287 |
| 3 | 50 | 15 | 3 | 256 | 61018 | 95 | 205 | 18.706148 | 1.0052 |

## Build Phase — SIFT

### Συμπεράσματα από τον Πίνακα A — Effect of m (αριθμός partitions)

- Η αύξηση του αριθμού των partitions **m** επηρεάζει το edgecut και την ισορροπία των blocks:  
  - m=50 → block sizes από 144 έως 205 → μεγαλύτερη διακύμανση  
  - m=150 → block sizes από 12 έως 68 → πιο ομοιόμορφη διαμέριση  
- Η τυπική απόκλιση μειώνεται όσο αυξάνεται το m → πιο balanced partitioning  
- Το final loss αυξάνεται όσο αυξάνεται το m, όπως και στο MNIST, λόγω περισσότερων blocks προς ταξινόμηση.

**Συμπέρασμα:**  
Υπάρχει trade-off μεταξύ ισορροπίας των blocks και ακρίβειας του MLP. Για το SIFT dataset φαίνεται ότι **m=100** προσφέρει ισορροπημένο αποτέλεσμα.

---

### Συμπεράσματα από τον Πίνακα B — Effect of kNN (μέγεθος k-NN graph)

- Αυξάνοντας το k:  
  - Το edgecut αυξάνεται σημαντικά (περισσότερες ακμές → περισσότερα κοψίματα)  
  - Τα block sizes παραμένουν σχεδόν ίδια (m=50)  
- Το final loss μειώνεται όσο αυξάνεται το k (k=5 → 1.4921, k=15 → 1.3208), δείχνοντας ότι το MLP επωφελείται από πιο πλούσιο γράφο.

**Συμπέρασμα:**  
k=15 προσφέρει **χαμηλότερο loss** και αποδεκτό κόστος partitioning.

---

### Συμπεράσματα Πίνακας C — Effect of Layers

- Με m=100, k=15:  
  - Edgecut και block sizes παραμένουν σταθερά  
  - Το MLP loss **αυξάνεται όσο προστίθενται layers**:  
    - 2 layers → 1.8692  
    - 3 layers → 1.8692  
    - 4 layers → 2.1411  

**Συμπέρασμα:**  
Όπως και στο MNIST, **πολύ βαθιά MLP δυσκολεύεται να μάθει σωστά τις KaHIP labels**. 2-3 layers είναι κατάλληλα.

---

### Συμπεράσματα Πίνακας D — Effect of Hidden Layer Size (Nodes)

- Με m=50, k=15, layers=3:  
  - Αύξηση των hidden nodes μειώνει το loss:  
    - 64 nodes → 1.7978  
    - 128 nodes → 1.3287  
    - 256 nodes → 1.0052  

**Συμπέρασμα:**  
Όπως στο MNIST, περισσότερα nodes βελτιώνουν την ταξινόμηση των blocks.

---

### 🔹 Γενικό Συμπέρασμα — Build Phase (SIFT)

- **m ≈ 100** για ισορροπημένο partitioning  
- **kNN = 15** για πλούσιο γράφο  
- **Layers = 2–3** για καλύτερη μάθηση  
- **Nodes = 256** για χαμηλότερο final loss  

Το pattern είναι **συμβατό με το MNIST**, με τη διαφορά ότι λόγω μικρότερου SIFT dataset (10.000 vectors) τα block sizes και το loss είναι πιο “σφιχτά” αριθμητικά.


---
## SEARCH PHASE
## Πίνακας A — Effect of N (requested neighbors)

Σταθερές παράμετροι: T = 5

| Run | N | AF | Recall@N | QPS | tApproxAvg | tTrueAvg |
|-----|---|------|------------|--------|-------------|-------------|
| 1 | 1  | 1.0 |0.984 |  464.2981281972148|0.0021537885665893555 |0.0038111352920532225 |
| 2 | 5  | 1.0 | 0.984 | 482.45715541149093 | 0.0020727229118347166 | 0.003243688344955444 |
| 3 | 10 |1.0 |0.984 |478.3959544824528 |0.0020903186798095705 | 0.003266164779663086 |

## Πίνακας B — Effect of T (bins to probe)

Σταθερές παράμετροι: N = 5

| Run | T | AF | Recall@10 | QPS | tApproxAvg | tTrueAvg |
|-----|----|--------|--------------|--------|----------------|----------------|
| 1 | 1  | 1.0 |0.739 | 1294.947538975281 |0.0007722320556640625 | 0.003324448823928833|
| 2 | 3  | 1.0 | 0.943 | 655.8725028729481 | 0.001524686574935913|0.003443063259124756 |
| 3 | 5  | 1.0 | 0.984 | 482.45715541149093 | 0.0020727229118347166 | 0.003243688344955444 |
| 4 | 10 | 1.0 |0.997 | 256.5846760434207 | 0.003897348880767822  |0.003372978210449219 |

# Search Phase — SIFT

## Συμπεράσματα Πίνακας A — Effect of N (requested neighbors)

- Το AF παραμένει **1.0** για όλες τις τιμές του N → η απόσταση των approximate neighbors είναι ιδανικά ίδια με την πραγματική.
- Το Recall@N είναι **0.984** για N=1,5,10 → το μοντέλο βρίσκει σχεδόν πάντα τον πραγματικό nearest neighbor.
- Ο QPS παραμένει **πολύ υψηλός** (>460) για όλα τα N, με μικρή πτώση όταν αυξάνεται το N (λόγω περισσότερων αποτελεσμάτων προς επιστροφή).
- Οι χρόνοι tApproxAvg και tTrueAvg είναι **πολύ μικροί**, δείχνοντας γρήγορη approximate και brute-force αναζήτηση.

**Συμπέρασμα:**  
Η επιλογή του N **δεν επηρεάζει ουσιαστικά την ακρίβεια ή την ταχύτητα**. Το Neural LSH βρίσκει σωστά τον nearest neighbor για όλα τα N.

---

## Συμπεράσματα Πίνακας B — Effect of T (bins to probe)

- Εδώ φαίνεται ξεκάθαρα το trade-off μεταξύ ταχύτητας και ακρίβειας:
  - **T=1 → Πάρα πολύ γρήγορο (QPS ~1295)** αλλά χαμηλό Recall@10 = 0.739  
  - **T=3 → Βελτίωση Recall 0.943**, QPS μειώνεται σε ~656  
  - **T=5 → Recall 0.984**, QPS ~482 → πολύ καλή ισορροπία  
  - **T=10 → Σχεδόν τέλεια ακρίβεια 0.997**, αλλά QPS πέφτει σε ~257

**Συμπέρασμα:**  
Το T=5 προσφέρει **το καλύτερο trade-off** για το SIFT dataset: υψηλή ακρίβεια με αποδεκτή ταχύτητα αναζήτησης.

---

### 🔹 Γενικό Συμπέρασμα — Search Phase (SIFT)

- Το Neural LSH **βρίσκει τον nearest neighbor με πολύ υψηλή ακρίβεια** (Recall@N ≥ 0.984).  
- Το AF είναι **ιδανικό (1.0)**, δηλαδή τα approximate distances είναι ακριβή.  
- Ο καλύτερος συνδυασμός παραμέτρων για το SIFT dataset είναι:  
  - **N = 5 ή 10** → δεν επηρεάζει ουσιαστικά την ακρίβεια  
  - **T = 5** → ισορροπία ακρίβειας / ταχύτητας



Συνολικά, ο Neural LSH εμφανίζεται **σαφώς ανώτερος** από τους αλγορίθμους της 1ης εργασίας τόσο σε ακρίβεια όσο και σε χρόνο αναζήτησης σε sift data.
---

Φτάνουμε σε αυτό το συμπέρασμα καθώς αν αναλύσουμε τα παρακάτω αποτελέσματα της 1ης εργασίας, τα οποία ήταν τα βέλτιστα για κάθε αλγόριθμο:

## LSH on SIFT 

| w | L | k | Average AF | Recall@N | QPS | tApproximate | tTrue |
|---|---|---|-----------:|---------:|----:|-------------:|------:|
200 | 15 | 2 | 1.002420 | 0.940000 | 2.135077 | 0.468367 | 1.227476

---

## Hypercube on SIFT 

| kproj | M | probes | w | Average AF | Recall@N | QPS | tApproximate | tTrue |
|------:|--:|-------:|--:|-----------:|---------:|----:|-------------:|------:|
18 | 50000 | 3000 | 150 | 1.018271 | 0.750000 | 11.406940 | 0.087666 | 1.298279

## Ivfflat on SIFT 

| N | kclusters | nprobe | seed | Average AF | Recall@N | QPS | tApproximateAverage (s) | tTrueAverage (s) |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 5 | 32 | 2 | 9 | 1.000920 | **0.922** | 8.963799 | 0.111560 | 1.254360 |

Παρατηρουμε οτι το βελτιστο του nlsh

## NLSH — SIFT

| Run | T | AF | Recall@10 | QPS | tApproxAvg | tTrueAvg |
|-----|----|--------|--------------|--------|----------------|----------------|
| 3 | 5  | 1.001247640490532 | 0.982 | 65.42223176118966 | 0.015285323858261108 | 0.09682433867454529 |

* Ανώτερο του LSH σε recall και ταχυτητα

* Ανώτερο του hypercube σε recall και ταχυτητα

* Ανώτερο του ivfflat σε recall και ταχυτητα

Επομένως, ο Neural LSH εμφανίζεται **σαφώς ανώτερος** από τους αλγορίθμους της 1ης εργασίας τόσο σε ακρίβεια όσο και σε χρόνο αναζήτησης στο sift.


---